{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd2a3b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from libribrain_experiments.grouped_dataset import MyGroupedDatasetV3\n",
    "from typing import Literal\n",
    "from pnpl.datasets import LibriBrainCompetitionHoldout, LibriBrainPhoneme\n",
    "\n",
    "raw_source_dataset = LibriBrainPhoneme(\n",
    "    data_path=\"./data/\",\n",
    "    tmin=0.0,\n",
    "    tmax=0.5,\n",
    "    standardize=True,\n",
    ")\n",
    "source_dataset = MyGroupedDatasetV3(\n",
    "    raw_source_dataset,\n",
    "    grouped_samples=100,\n",
    "    drop_remaining=False,\n",
    "    average_grouped_samples=True,\n",
    "    state_cache_path=Path(f\"./data_preprocessed/groupedv3/all_grouped_100.pt\"),\n",
    "    # balance=True,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "holdout_dataset = LibriBrainCompetitionHoldout(\n",
    "    data_path=\"./data/\",\n",
    "    task=\"phoneme\",\n",
    "    tmin=0.0,\n",
    "    tmax=0.5,\n",
    "    standardize=False, # already standardized\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fa73cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "\n",
    "full_dataset = ConcatDataset([source_dataset, holdout_dataset])\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "def collate_fn(batch):\n",
    "    ys = torch.empty((len(batch),), dtype=torch.long)\n",
    "    for i, sample in enumerate(batch):\n",
    "        # print(f\"{i}: {type(sample)}\")\n",
    "        if type(sample) is tuple:\n",
    "            ys[i] = 0  # from source_dataset\n",
    "            batch[i] = sample[0]\n",
    "        else:\n",
    "            ys[i] = 1  # from holdout_dataset\n",
    "    xs = torch.stack(batch)\n",
    "    return xs, ys\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3191f8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import lightning as L\n",
    "from torch import nn\n",
    "from torchmetrics import F1Score\n",
    "\n",
    "\n",
    "class SourceClassificationModel(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv1d(306, 128, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16000, 2)\n",
    "        )\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.f1_macro = F1Score(\n",
    "            num_classes=2, average='macro', task=\"multiclass\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        f1_macro = self.f1_macro(y_hat, y)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        self.log('train_f1_macro', f1_macro)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        f1_macro = self.f1_macro(y_hat, y)\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_f1_macro', f1_macro, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "870b47ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/dogeon/libribrain/phoneme/.venv/lib/python3.12/site-packages/torch/__init__.py:1539: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  return _C._get_float32_matmul_precision()\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 5090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type              | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | model     | Sequential        | 71.3 K | train\n",
      "1 | criterion | CrossEntropyLoss  | 0      | train\n",
      "2 | f1_macro  | MulticlassF1Score | 0      | train\n",
      "--------------------------------------------------------\n",
      "71.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "71.3 K    Total params\n",
      "0.285     Total estimated model params size (MB)\n",
      "7         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23a30013192d4c9b9f024cbcff84dcd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dogeon/libribrain/phoneme/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "/home/dogeon/libribrain/phoneme/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096bbb32ff8f4474884a895bf670a8ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4323ee3dad420c982d76087c1b1a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d97abc76940c453e87246f2d073d62b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dogeon/libribrain/phoneme/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import TensorBoardLogger, CSVLogger\n",
    "\n",
    "# Setup paths for logs and checkpoints\n",
    "LOG_DIR = f\"lightning_logs\"\n",
    "CHECKPOINT_PATH = f\"models/phoneme_model.ckpt\"\n",
    "\n",
    "logger = CSVLogger(\n",
    "    save_dir=LOG_DIR,\n",
    "    name=\"\",\n",
    "    version=None,\n",
    ")\n",
    "\n",
    "# Set a fixed seed for reproducibility\n",
    "L.seed_everything(42)\n",
    "\n",
    "# Conditionally set num_workers to avoid multiprocessing issues (try increasing if performance is problematic)\n",
    "num_workers = 4\n",
    "\n",
    "# Initialize the SourceClassificationModel model\n",
    "model = SourceClassificationModel()\n",
    "\n",
    "# Log Hyperparameters (these will be empty be default!)\n",
    "logger.log_hyperparams(model.hparams)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = L.Trainer(\n",
    "    devices=\"auto\",\n",
    "    max_epochs=15,\n",
    "    logger=logger,\n",
    "    enable_checkpointing=True,\n",
    ")\n",
    "\n",
    "# Actually train the model\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "# Save the trained model\n",
    "trainer.save_checkpoint(CHECKPOINT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "libribrain-phoneme (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
