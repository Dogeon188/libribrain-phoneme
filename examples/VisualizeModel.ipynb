{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e68f3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "checkpoint_path = \"out/phoneme-baseline-xl/best-val_bal_acc-baseline-xl-hpo-2-epoch=08-val_f1_macro=0.7019.ckpt\"\n",
    "# checkpoint_path = \"out/phoneme-megt/best-val_bal_acc-megt-s-hpo-0-epoch=09-val_f1_macro=0.6605.ckpt\"\n",
    "\n",
    "split: Literal[\"train\", \"val\"] = \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55a6c39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from libribrain_experiments.models.configurable_modules.classification_module import ClassificationModule\n",
    "from pnpl.datasets.libribrain2025 import constants_utils\n",
    "\n",
    "constants_utils.set_remote_constants_url(\n",
    "    f\"{(Path(os.getcwd()) / 'constants.json').as_uri()}\")\n",
    "constants_utils.refresh_constants()\n",
    "model = ClassificationModule.load_from_checkpoint(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89e2c828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   | Name                         | Type                | Params | Mode \n",
       "------------------------------------------------------------------------------\n",
       "0  | modules_list                 | ModuleList          | 6.1 M  | train\n",
       "1  | modules_list.0               | Conv1d              | 548 K  | train\n",
       "2  | modules_list.1               | ResnetBlock         | 262 K  | train\n",
       "3  | modules_list.1.module_list   | ModuleList          | 262 K  | train\n",
       "4  | modules_list.1.module_list.0 | ELU                 | 0      | train\n",
       "5  | modules_list.1.module_list.1 | Conv1d              | 196 K  | train\n",
       "6  | modules_list.1.module_list.2 | ELU                 | 0      | train\n",
       "7  | modules_list.1.module_list.3 | Conv1d              | 65.8 K | train\n",
       "8  | modules_list.2               | ELU                 | 0      | train\n",
       "9  | modules_list.3               | Conv1d              | 196 K  | train\n",
       "10 | modules_list.4               | ResnetBlock         | 262 K  | train\n",
       "11 | modules_list.4.module_list   | ModuleList          | 262 K  | train\n",
       "12 | modules_list.4.module_list.0 | ELU                 | 0      | train\n",
       "13 | modules_list.4.module_list.1 | Conv1d              | 196 K  | train\n",
       "14 | modules_list.4.module_list.2 | ELU                 | 0      | train\n",
       "15 | modules_list.4.module_list.3 | Conv1d              | 65.8 K | train\n",
       "16 | modules_list.5               | ELU                 | 0      | train\n",
       "17 | modules_list.6               | Conv1d              | 3.3 M  | train\n",
       "18 | modules_list.7               | ELU                 | 0      | train\n",
       "19 | modules_list.8               | Conv1d              | 459 K  | train\n",
       "20 | modules_list.9               | ELU                 | 0      | train\n",
       "21 | modules_list.10              | Conv1d              | 1.0 M  | train\n",
       "22 | modules_list.11              | ReLU                | 0      | train\n",
       "23 | modules_list.12              | Dropout             | 0      | train\n",
       "24 | modules_list.13              | Flatten             | 0      | train\n",
       "25 | modules_list.14              | Linear              | 40.0 K | train\n",
       "26 | loss_fn                      | CrossEntropyLoss    | 0      | train\n",
       "27 | accuracy                     | MulticlassAccuracy  | 0      | train\n",
       "28 | balanced_accuracy            | MulticlassAccuracy  | 0      | train\n",
       "29 | f1_micro                     | MulticlassF1Score   | 0      | train\n",
       "30 | f1_macro                     | MulticlassF1Score   | 0      | train\n",
       "31 | precision_micro              | MulticlassPrecision | 0      | train\n",
       "32 | precision_macro              | MulticlassPrecision | 0      | train\n",
       "33 | binary_accuracy              | BinaryAccuracy      | 0      | train\n",
       "34 | binary_precision             | BinaryPrecision     | 0      | train\n",
       "35 | binary_recall                | BinaryRecall        | 0      | train\n",
       "36 | binary_f1                    | BinaryF1Score       | 0      | train\n",
       "------------------------------------------------------------------------------\n",
       "6.1 M     Trainable params\n",
       "0         Non-trainable params\n",
       "6.1 M     Total params\n",
       "24.386    Total estimated model params size (MB)\n",
       "37        Modules in train mode\n",
       "0         Modules in eval mode"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightning.pytorch.utilities.model_summary import ModelSummary\n",
    "\n",
    "ModelSummary(model, max_depth=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89373031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pnpl.datasets import LibriBrainPhoneme\n",
    "from libribrain_experiments.grouped_dataset import MyGroupedDatasetV3\n",
    "\n",
    "raw_visualization_dataset = LibriBrainPhoneme(\n",
    "    data_path=\"./data/\",\n",
    "    tmin=0.0,\n",
    "    tmax=0.5,\n",
    "    standardize=True,\n",
    "    partition=\"train\" if split == \"train\" else \"validation\",\n",
    ")\n",
    "visualization_dataset = MyGroupedDatasetV3(\n",
    "    raw_visualization_dataset,\n",
    "    grouped_samples=100,\n",
    "    drop_remaining=False,\n",
    "    average_grouped_samples=True,\n",
    "    state_cache_path=Path(f\"./data_preprocessed/groupedv3/{split}_grouped_100.pt\"),\n",
    "    balance=True,\n",
    "    # repeat=20,\n",
    "    shuffle=True,\n",
    "    # augment=True,  # Set to True if you want to apply data augmentation\n",
    ")\n",
    "\n",
    "# raw_visualization_dataset = LibriBrainPhoneme(\n",
    "#     data_path=\"./data/\",\n",
    "#     tmin=0.0,\n",
    "#     tmax=0.5,\n",
    "#     standardize=True,\n",
    "#     partition=\"validation\",\n",
    "# )\n",
    "# visualization_dataset = MyGroupedDatasetV3(\n",
    "#     raw_visualization_dataset,\n",
    "#     grouped_samples=100,\n",
    "#     drop_remaining=False,\n",
    "#     average_grouped_samples=True,\n",
    "#     state_cache_path=Path(\"./data_preprocessed/groupedv3/val_grouped_100.pt\"),\n",
    "#     balance=True,\n",
    "#     # repeat=20,\n",
    "#     shuffle=True,\n",
    "#     # augment=True,  # Set to True if you want to apply data augmentation\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bac3a566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac62399abf9c4c89bf3692fa191c3b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(ToggleButtons(description='Layer:', index=14, options=('layer_0', 'layer_1', 'laâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dogeon/libribrain/phoneme/.venv/lib/python3.12/site-packages/jupyter_client/session.py:721: UserWarning:\n",
      "\n",
      "Message serialization failed with:\n",
      "Out of range float values are not JSON compliant: nan\n",
      "Supporting this message is deprecated in jupyter-client 7, please make sure your message is JSON-compliant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import plotly.graph_objs as go\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "# Sample & collect activations\n",
    "\n",
    "activations = {}\n",
    "\n",
    "sample_id = np.random.randint(0, len(visualization_dataset))\n",
    "sample = visualization_dataset[sample_id][0].unsqueeze(0)\n",
    "sample_label = visualization_dataset[sample_id][1].item()\n",
    "sample_label_phoneme = raw_visualization_dataset.id_to_phoneme[sample_label]\n",
    "\n",
    "\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activations[name] = output.detach().cpu()\n",
    "    return hook\n",
    "\n",
    "\n",
    "# Register hooks for each layer in modules_list\n",
    "for idx, layer in enumerate(model.modules_list):\n",
    "    layer.register_forward_hook(get_activation(f'layer_{idx}'))\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    _ = model.forward(sample.to(model.device))\n",
    "\n",
    "\n",
    "activations_per_row = 32\n",
    "\n",
    "# Register hooks for each layer in modules_list\n",
    "for idx, layer in enumerate(model.modules_list):\n",
    "    layer.register_forward_hook(get_activation(f'layer_{idx}'))\n",
    "\n",
    "plot_fig: go.FigureWidget = go.FigureWidget()\n",
    "plot_fig_input: go.FigureWidget = go.FigureWidget()\n",
    "\n",
    "\n",
    "# Plotting functions\n",
    "\n",
    "def plot_input(sample):\n",
    "    sample_np = sample.squeeze().cpu().numpy()\n",
    "    plot_fig_input.data = []  # Clear previous traces\n",
    "    if sample_np.ndim == 2:  # (channels, time)\n",
    "        trace = go.Heatmap(z=sample_np,\n",
    "                           colorscale='Viridis', name='Input',\n",
    "                           colorbar=dict(title='Amplitude'))\n",
    "        plot_fig_input.add_trace(trace)\n",
    "        plot_fig_input.update_layout(\n",
    "            title=f'Input Signal #{sample_id} - /{sample_label_phoneme}/ ({sample_label})',\n",
    "            xaxis_title='Time', yaxis_title='Channel Index',\n",
    "            # Set figure size to match plot\n",
    "            width=5 * sample_np.shape[1], height=2.5 * sample_np.shape[0]\n",
    "        )\n",
    "    else:\n",
    "        plot_fig_input.update_layout(title='Input (Unsupported shape)')\n",
    "    plot_fig_input.show()\n",
    "\n",
    "\n",
    "def plot_activation(layer_name):\n",
    "    act = activations[layer_name]\n",
    "    act_np = act.squeeze().cpu().numpy()\n",
    "    plot_fig.data = []  # Clear previous traces\n",
    "    if act_np.ndim == 2:  # (channels, time)\n",
    "        plot_height = max(300, 2.5 * act_np.shape[0])\n",
    "        plot_width = min(800, max(500, 5 * act_np.shape[1]))\n",
    "\n",
    "        trace = go.Heatmap(z=act_np,\n",
    "                           colorscale='Viridis', name=f'Convolutional Layer',\n",
    "                           colorbar=dict(title='Activation'))\n",
    "\n",
    "        plot_fig.add_trace(trace)\n",
    "        plot_fig.update_layout(\n",
    "            title=f'{layer_name} - Convolutional Layer',\n",
    "            yaxis=dict(title='Channel Index', tickmode='linear',\n",
    "                       dtick=min(50, act_np.shape[0] // 10)),\n",
    "            xaxis=dict(title='Time', tickmode='linear',\n",
    "                       dtick=min(20, act_np.shape[1] // 10)),\n",
    "            width=plot_width, height=plot_height\n",
    "        )\n",
    "    elif act_np.ndim == 1:  # (features,)\n",
    "        is_final_layer = act_np.shape[0] == 39\n",
    "\n",
    "        # Pad act_np if not a multiple of activations_per_row\n",
    "        pad_len = (-act_np.size % activations_per_row)\n",
    "        if pad_len != 0:\n",
    "            act_np = np.copy(act_np)  # Ensure we don't modify the original\n",
    "            act_np = np.pad(act_np, (0, pad_len), constant_values=np.nan)\n",
    "        act_np = act_np.reshape(-1, activations_per_row)\n",
    "\n",
    "        plot_height = max(400, 25 * act_np.shape[0])\n",
    "        plot_width = 25 * activations_per_row\n",
    "\n",
    "        trace = go.Heatmap(z=act_np,\n",
    "                           colorscale='Viridis', name='Features')\n",
    "        plot_fig.add_trace(trace)\n",
    "\n",
    "        if is_final_layer:\n",
    "            # fill a square grid indexed by predicted_label\n",
    "            predicted_label = np.nanargmax(act_np)\n",
    "            predicted_label_x = predicted_label % activations_per_row\n",
    "            predicted_label_y = predicted_label // activations_per_row\n",
    "            trace = go.Scatter(\n",
    "                x=[predicted_label_x - 0.5, predicted_label_x + 0.5, predicted_label_x +\n",
    "                    0.5, predicted_label_x - 0.5, predicted_label_x - 0.5],\n",
    "                y=[predicted_label_y - 0.5, predicted_label_y - 0.5, predicted_label_y +\n",
    "                    0.5, predicted_label_y + 0.5, predicted_label_y - 0.5],\n",
    "                mode='lines',\n",
    "                line=dict(color='red', width=0),\n",
    "                fill='toself',\n",
    "                fillpattern=dict(\n",
    "                    shape='x',\n",
    "                    solidity=0.4,\n",
    "                    size=6,\n",
    "                    fgcolor='red',\n",
    "                    bgcolor='rgba(0,0,0,0.1)'  # Semi-transparent background\n",
    "                ),\n",
    "                name='Predicted Label Index',\n",
    "                zorder=10\n",
    "            )\n",
    "            plot_fig.add_trace(trace)\n",
    "\n",
    "            # draw a red square outline around the grid indexed by sample_label\n",
    "            sample_label_x = sample_label % activations_per_row\n",
    "            sample_label_y = sample_label // activations_per_row\n",
    "            trace = go.Scatter(\n",
    "                x=[sample_label_x - 0.5, sample_label_x + 0.5, sample_label_x +\n",
    "                    0.5, sample_label_x - 0.5, sample_label_x - 0.5],\n",
    "                y=[sample_label_y - 0.5, sample_label_y - 0.5, sample_label_y +\n",
    "                    0.5, sample_label_y + 0.5, sample_label_y - 0.5],\n",
    "                mode='lines',\n",
    "                line=dict(color='red', width=2),\n",
    "                name='Sample Label Index',\n",
    "                zorder=10\n",
    "            )\n",
    "            plot_fig.add_trace(trace)\n",
    "\n",
    "        plot_fig.update_layout(\n",
    "            title=f'{layer_name} - Dense' +\n",
    "            f' (Final, {predicted_label}?{sample_label}!)' if is_final_layer else '',\n",
    "            xaxis=dict(title='Feature Index', tickmode='linear', dtick=4),\n",
    "            yaxis=dict(title='', tickmode=\"array\",\n",
    "                       tickvals=np.arange(0, act_np.shape[0], 4),\n",
    "                       ticktext=np.arange(0, act_np.shape[0], 4) * activations_per_row),\n",
    "            xaxis_tickangle=0,\n",
    "            width=plot_width, height=plot_height)\n",
    "    else:\n",
    "        plot_fig.update_layout(title=f'{layer_name} (Unsupported shape)')\n",
    "    plot_fig.show()\n",
    "\n",
    "\n",
    "layer_selector = widgets.ToggleButtons(\n",
    "    options=list(activations.keys()),\n",
    "    value=list(activations.keys())[-1],\n",
    "    description='Layer:'\n",
    ")\n",
    "\n",
    "\n",
    "def on_change(change):\n",
    "    plot_activation(layer_selector.value)\n",
    "\n",
    "\n",
    "layer_selector.observe(on_change, names='value')\n",
    "\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([layer_selector, ]),\n",
    "    widgets.HBox([plot_fig_input, plot_fig])\n",
    "]))\n",
    "plot_input(sample)\n",
    "plot_activation(layer_selector.value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "libribrain-phoneme (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
